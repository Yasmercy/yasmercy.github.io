<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" href="assets/css/bootstrap.min.css ">
        <link rel="stylesheet" href="assets/css/code.min.css">
        <link rel="stylesheet" href="assets/css/atom-one-dark-reasonable.css">
        <link rel="stylesheet" href="assets/css/style.css" />
        <link rel='stylesheet' href='assets/css/roboto.css'>
        <script type="text/javascript" src="assets/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <title>Chapter One</title>
    </head>
    <body>

        <div class="container">
            
<div id="Overview"><h1 id="Overview" class="header"><a href="#Overview">Overview</a></h1></div>
<p>
We want to find the optimal policy \(\pi\) for a finite markov decision process (MDP)
</p>

<div id="Overview-Markov Decision Process"><h2 id="Markov Decision Process" class="header"><a href="#Overview-Markov Decision Process">Markov Decision Process</a></h2></div>

<p>
We have a feedback loop as such
</p>
<ul>
<li>
<span id="Overview-Markov Decision Process-Agent"></span><strong id="Agent">Agent</strong> \(\xrightarrow{S_t, A_t}\) <span id="Overview-Markov Decision Process-Environment"></span><strong id="Environment">Environment</strong> \(\xrightarrow{S_{t+1}, R_{t+1}}\) <span id="Overview-Markov Decision Process-Agent"></span><strong id="Agent">Agent</strong> \(\xrightarrow{\ldots} \ldots\)

</ul>
<p>
In particular, we may define <em>policies</em> \(\pi(a \mid s)\) that determine what <em>actions</em> the <em>agent</em> takes at each <em>state</em>.
</p>

<p>
Then, we will try and maximize the <em>discounted return</em> \(G^{(\pi)}_t = \sum_{k=t+1}^T \gamma^{k - {t + 1}} R_k\).
Note that because these are probabilistic policies, we want \(\mathrm{max}_\pi \mathbb{E}[G^{(\pi)}_t]\).
</p>

<p>
Finally, it is useful to define the value functions that we will be working with
</p>
<ul>
<li>
<span id="Overview-Markov Decision Process-State Value"></span><strong id="State Value">State Value</strong> \(v_\pi(s) = \mathbb{E}[G^{(\pi)}_t \mid S_t = s]\)

<li>
<span id="Overview-Markov Decision Process-Action Value"></span><strong id="Action Value">Action Value</strong> \(v_\pi(s, a) = \mathbb{E}[G^{(\pi)}_t \mid S_t = s, A_t = a]\)

</ul>
<div id="Overview-Example Episode"><h2 id="Example Episode" class="header"><a href="#Overview-Example Episode">Example Episode</a></h2></div>

<p>
We shall simulate one possible example of a (deterministic) Markov Decision Process. 
</p>

<p>
Suppose we have \(\mathcal{S} = \{1, 2, 3\}\), \(\mathcal{A} = \{1, 2\}\).
Further, suppose our reward function \(p(R_t = r \mid S_t = s, A_t = a)\) is as follows
</p>
<ul>
<li>
\(p(-1 \mid s, a) = \frac{s + a}{5}\)

<li>
\(p(+1 \mid s, a) = 1 - p(-1 \mid s, a)\)

</ul>
<p>
And our state transitions are 
</p>
<ul>
<li>
\(s \xrightarrow{a} s + 1 \ (\mathrm{mod} \ 3)\)

<li>
\(s \xrightarrow{b} s - 1 \ (\mathrm{mod} \ 3)\)

</ul>
<p>
We can set our starting distibution to be uniform.
Finally, we define the terminal state to occur after \(N\) time steps.
</p>

<p>
For example, we may choose a deterministic policy \(\pi(1 \mid s) = 1\), and \(\pi(2 \mid s) = 0\).
</p>
<ul>
<li>
\(1 \xrightarrow{1} 2 \xrightarrow{1} 3 \xrightarrow{1} 1 \xrightarrow{1} \ldots \xrightarrow{1} 1 + N \ (\mathrm{mod} \ 3)\)

<li>
\(2 \xrightarrow{1} \ldots \xrightarrow{1} 2 + N \ (\mathrm{mod} \ 3)\)

<li>
\(3 \xrightarrow{1} \ldots \xrightarrow{1} N \ (\mathrm{mod} \ 3)\)

</ul>
<p>
each with probability \(\frac{1}{3}\).
</p>

<p>
<a href="Reinforcement Learning.html">Home</a> <a href="Chapter One.html">1</a> <a href="Chapter Two.html">2</a> <a href="Chapter Three.html">3</a> <a href="Chapter Four.html">4</a><a href="Eligibility Traces.html">(b)</a> <a href="Chapter Five.html">5</a> <a href="Chapter Six.html">6</a> <a href="Chapter Two.html">Next</a>
</p>

        </div>
    </body>
