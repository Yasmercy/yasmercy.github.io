<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" href="assets/css/bootstrap.min.css ">
        <link rel="stylesheet" href="assets/css/code.min.css">
        <link rel="stylesheet" href="assets/css/atom-one-dark-reasonable.css">
        <link rel="stylesheet" href="assets/css/style.css" />
        <link rel='stylesheet' href='assets/css/roboto.css'>
        <script type="text/javascript" src="assets/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <title>Chapter Four</title>
    </head>
    <body>
        <div class="container">
            
<div id="TD Learning"><h2 id="TD Learning" class="header"><a href="#TD Learning">TD Learning</a></h2></div>

<p>
In this section, we further refine our \(\alpha\)-MC.
In particular, we shall study SARSA and Q-learning, that may handle infinite episode lengths.
</p>

<div id="Bootstrapping"><h2 id="Bootstrapping" class="header"><a href="#Bootstrapping">Bootstrapping</a></h2></div>

<p>
Recall the update rule used in \(\alpha\)-MC
</p>
\[
    V(s_t) \xleftarrow{} V(s_t) + \alpha(g^{(m)}_t - V(s_t))
\]

<p>
We have this \(g^{(m)}_t\) term, that requires knowing the reward until the end of the episode.
Bootstrapping refers to the technique of estimating \(g_t\) using our existing value estimate.
</p>

<p>
In particular, we have that 
</p>
\[
    g_t \approx \sum_{k=1}^{T-t} \gamma^k R_{k+t}
        \approx \sum_{k=1}^{min(T - t, n)} \gamma^k R_{k+t} + \gamma^n V(s_{t+n})
\]

<p>
where instead of delaying until the end of the episode, we delay \(n\) steps before updating the rewards.
</p>

<div id="SARSA and Q-Learning"><h2 id="SARSA and Q-Learning" class="header"><a href="#SARSA and Q-Learning">SARSA and Q-Learning</a></h2></div>

<p>
We may now discuss three algorithms implementing of TD learning.
</p>

<p>
First, n-step SARSA, (\(s_t, a_t, r_t, s_{t+1}, a_{t+1}\)), directly applies the TD formula into a MRP (states \(\leftarrow\) state-action <em>pairs</em>).
</p>
\begin{align}
    g^{(m)}_t &amp;\xleftarrow{} \sum_{k=1}^{min(T - t, n)} \gamma^k R^{(m)}_{k+t} + \gamma^n v(s^{(m)}_{t+n}, a^{(m)}_{t+n}) \\
    v(s^{(m)}_t, a^{(m)}_t) &amp;\xleftarrow{} (1 - \alpha) v(s^{(m)}_t, a^{(m)}_t) + \alpha g^{(m)}_t
\end{align}

<p>
To reduce the sampling error, we may take the following goal functions, used in <em>expected SARSA</em> and <em>Q-learning</em> respectively
</p>
\begin{align}
    g^{(m)}_t &amp;\xleftarrow{} \sum_{k=1}^{min(T - t, n)} \gamma^k R^{(m)}_{k+t} + \gamma^n \sum_a v(s^{(m)}_{t+n}, a) \pi(a \mid s^{(m)}_{t+n}) \\
    g^{(m)}_t &amp;\xleftarrow{} \sum_{k=1}^{min(T - t, n)} \gamma^k R^{(m)}_{k+t} + \gamma^n \mathrm{max}_a v(s^{(m)}_{t+n}, a)
\end{align}

<div id="Evaluation"><h2 id="Evaluation" class="header"><a href="#Evaluation">Evaluation</a></h2></div>

<p>
\(\alpha\)-MC is an algorithm that updates the values linearly to the difference in rewards.
This is exactly what the method for minimizing least sqaures does:
</p>
<ul>
<li>
\(E(x, \hat{x}) = \sum (x - \hat{x})^2\)

<li>
\(E'(x, \hat{x}) = \sum 2 (x - \hat{x})\)

<li>
\(\hat{x} \xleftarrow{} \hat{x} + \alpha E'(x, \hat{x}) = \hat{x} + \alpha (x - \hat{x})\)

</ul>
<p>
However, (1-step) TD learning finds the method of moments estimator (same as MLE) of the reward.
</p>
<ul>
<li>
\(\hat{p}(s', r | s) = \frac{C(s', r \mid s)}{C(s)}\)

</ul>
<p>
Formally, these analysis only work on <em>batched</em> learning, but the implementation simply approximates the batched, so we expect similar results.
As TD more directly targets the reward, it generally achieves a faster or stronger convergence.
</p>

<div id="SARSA vs Q-Learning"><h2 id="SARSA vs Q-Learning" class="header"><a href="#SARSA vs Q-Learning">SARSA vs Q-Learning</a></h2></div>
<p>
The <span id="SARSA vs Q-Learning-Cliff Walking"></span><strong id="Cliff Walking">Cliff Walking</strong> example shown in the <a href="https://www.youtube.com/watch?v=AJiG3ykOxmY&amp;list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr&amp;index=5">video</a> is particuarly insightful.
</p>

<p>
Using an \(\epsilon\)-Greedy process, SARSA recognizes that standing near the cliff is very risky.
</p>

<p>
However, as \(Q\)-learning does not have this \(epsilon\) chance of walking into the cliff, it is able to learn the optimal path.
This shows the strength of <em>off policy</em> methods, taking advantage of importance sampling.
</p>

<p>
Further, expected SARSA picks somewhere in the middle.
</p>

<div id="Eligibility Traces"><h2 id="Eligibility Traces" class="header"><a href="#Eligibility Traces">Eligibility Traces</a></h2></div>
<p>
See <a href="Eligibility Traces.html">Eligibility Traces</a> for a modification to the existing MC/TD algorithms to improve learning efficiency.
</p>

<p>
<a href="Reinforcement Learning.html">Home</a> <a href="Chapter One.html">1</a> <a href="Chapter Two.html">2</a> <a href="Chapter Three.html">3</a> <a href="Chapter Four.html">4</a><a href="Eligibility Traces.html">(b)</a> <a href="Chapter Five.html">5</a> <a href="Chapter Six.html">6</a> <a href="Chapter Five.html">Next</a>
</p>

        </div>
    </body>
