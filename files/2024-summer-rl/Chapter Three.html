<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" href="assets/css/bootstrap.min.css ">
        <link rel="stylesheet" href="assets/css/code.min.css">
        <link rel="stylesheet" href="assets/css/atom-one-dark-reasonable.css">
        <link rel="stylesheet" href="assets/css/style.css" />
        <link rel='stylesheet' href='assets/css/roboto.css'>
        <script type="text/javascript" src="assets/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <title>Chapter Three</title>
    </head>
    <body>

        <div class="container">
            
<div id="Assumptions"><h2 id="Assumptions" class="header"><a href="#Assumptions">Assumptions</a></h2></div>

<p>
Although we now have a method for "solving" RL problems, we need to mention the hidden attached strings.
</p>
<ul>
<li>
We required perfect information about our system

<li>
Our method is extremely computationally intensive

</ul>
<p>
Now, we shall focus on improving these processes.
</p>

<div id="Model Free"><h2 id="Model Free" class="header"><a href="#Model Free">Model Free</a></h2></div>
<p>
First, we relax the assumption that we need to know \(q(s', r | s, a)\).
For example, in the <a href="Gambler.html">Gambler's game</a>, now the weighting of the coin is unknown.
</p>

<p>
We have two options
</p>
<ul>
<li>
We may try and estimate the probabilities of each transition (model based)

<li>
We may go straight towards estimating the value function of actions (model free)

</ul>
<p>
In this section, we explore the latter.
</p>

<div id="$\alpha$-Monte Carlo"><h2 id="$\alpha$-Monte Carlo" class="header"><a href="#$\alpha$-Monte Carlo">\(\alpha\)-Monte Carlo</a></h2></div>

<p>
To estimate the \(v_\pi(s, a)\), we must first take <em>trajectories</em>.
</p>
\[
    (s_0, a_0)^m \xrightarrow{r^m_0} (s_1, a_1)^m \xrightarrow{r^m_1} \ldots \xrightarrow{r^m_{T-1}} (s_T, a_T)^{m}
\]
<p>
Note that now we have joined together \(s^m_{i, \mathrm{new}} := (s_i, a_i)^m\), the \(i\)-th state in the \(m\)-th trajectory.
</p>

<p>
Now, we have that the value of a state is the average reward across the trajectories, or that
</p>
\[
    v_\pi(s) \approx \frac{1}{C(s)} \sum_{m=1}^M \sum_{t=0}^{T_m - 1} \mathbb{I}[s^m_t = s] g^m_t
\]
<p>
where \(C(s)\) is the total count of visits to \(s\) in all \(M\) trajectories.
</p>

<p>
As a further approximation, we may designate \(\alpha \approx C(s)\), the <em>learning rate</em>.
This allows us to update \(v_\pi(s)\) incrementally, after each trajectory.
</p>
\[
    V(s_t) \xleftarrow{} V(s_t) + \alpha(g^{(m)}_t - V(s_t))
\]

<p>
Thus, after obtaining our value function, we may take the simple policy \(\pi'(s) = \mathrm{argmax_a} v_\pi(s, a)\)
</p>

<div id="Exploration"><h2 id="Exploration" class="header"><a href="#Exploration">Exploration</a></h2></div>
<p>
Suppose we had a bad initial policy (very reasonable) that did not explore every state.
Then, if this unexplored state was part of the optimal strategy, then we would never be optimal.
</p>

<p>
In general, we have two conflicting ideas
</p>
<ul>
<li>
Exploitation: Maximizing returns means choosing the best path

<li>
Exploration: Optimal policies require visiting <em>all</em> paths

</ul>
<p>
Thus, we describe the simplest \(\epsilon\)-greedy policy, where with probability epsilon, we take a random action, and otherwise we take the highest return action.
</p>

<div id="Off Policy"><h2 id="Off Policy" class="header"><a href="#Off Policy">Off Policy</a></h2></div>
<p>
Finally, we further generalize \(\alpha\)-Monte Carlo.
In particular, we allow for the data collection policy \(b\) to differ from our decision policy \(pi\), applying <em>importance sampling</em>.
</p>

<p>
Skipping the derivation, the only modification to the algorithm is to discount the reward \(g_t\) by a correlation coefficient \(\rho(b, \pi)\)
</p>
\[
    g^{(m)}_t = \rho^m_t(b, \pi)(r^{(m)}_{t+1} + \gamma r^{(m)}_{t+2} + \ldots)
\]
<p>
where 
</p>
\[
    \rho^{(m)}_t(b, \pi) = \prod_{\tau = t + 1}^{T_m} \frac{\pi(a^{(m)}_\tau | s^{(m)}_\tau)}{b(a^{(m)}_\tau | s^{(m)}_\tau)}
\]

<p>
This allows us to change \(b\) independently of \(\pi\), for example by doing parallel trajectories all under different \(b\) policies.
</p>

<div id="BlackJack"><h2 id="BlackJack" class="header"><a href="#BlackJack">BlackJack</a></h2></div>

<p>
An implementation of an RL algorithm to solve blackjack is <a href="vfile:~/src/learning/blackjack/">here</a>.
</p>

<p>
<a href="Reinforcement Learning.html">Home</a> <a href="Chapter One.html">1</a> <a href="Chapter Two.html">2</a> <a href="Chapter Three.html">3</a> <a href="Chapter Four.html">4</a><a href="Eligibility Traces.html">(b)</a> <a href="Chapter Five.html">5</a> <a href="Chapter Six.html">6</a> <a href="Chapter Four.html">Next</a>
</p>

        </div>
    </body>
