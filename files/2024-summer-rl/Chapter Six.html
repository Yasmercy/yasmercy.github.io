<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" href="assets/css/bootstrap.min.css ">
        <link rel="stylesheet" href="assets/css/code.min.css">
        <link rel="stylesheet" href="assets/css/atom-one-dark-reasonable.css">
        <link rel="stylesheet" href="assets/css/style.css" />
        <link rel='stylesheet' href='assets/css/roboto.css'>
        <script type="text/javascript" src="assets/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <title>Chapter Six</title>
    </head>
    <body>
        <div class="container">
            
<div id="Policy Gradient Methods (PGM)"><h2 id="Policy Gradient Methods (PGM)" class="header"><a href="#Policy Gradient Methods (PGM)">Policy Gradient Methods (PGM)</a></h2></div>

<p>
In this article, we shall primarily focus on the intuition and implementation of the \(\rm{R}{\small EINFORCE}\) algorithm.
</p>

<p>
First, from last time, we discuseed how to paramaterize the value function \(\hat{v}(s, w)\).
Now, we shall directly parameterize \(\pi(s, \theta)\).
However, instead of trying to minimize some least-squared-error, we shall instead try to maximize rewards \(\theta \xleftarrow{} \mathrm{argmax}_\theta \ g(\pi, \theta)\).
</p>

<p>
Then, our update rule becomes
</p>
\[
    \theta \xleftarrow{} \theta + \alpha g^{(m)}_t \nabla_\theta \ln (\pi(a^{(m)}_t \mid s^{(m)}_t, \theta))
\]
<p>
note that the \(\ln\) is present in order to normalize the rewards based on the probability of selecting an action.
</p>

<div id="Baseline"><h2 id="Baseline" class="header"><a href="#Baseline">Baseline</a></h2></div>

<p>
One issue with maximizing rewards is that the absolute magnitudes of the rewards do not matter.
This means that, for example, adding a scalar \(c\) to all rewards would cluster their <em>relative</em> magnitudes.
Then, as it is harder to decide between rewards that are all similar, it would slow down convergence to an optimum.
</p>

<p>
Thus, we shall introduce a <em>bias</em> term, in order to try and center the expected rewards.
In particular, one natural choice is to reintroduce our (parameterized) value estimate \(\hat{v}(s, w)\).
</p>
\[
    \theta \xleftarrow{} \theta + \alpha [g^{(m)}_t - \hat{v}(s, w)] \nabla_\theta \ln (\pi(a^{(m)}_t \mid s^{(m)}_t, \theta))
\]

<div id="Convergence Theorem"><h2 id="Convergence Theorem" class="header"><a href="#Convergence Theorem">Convergence Theorem</a></h2></div>

<p>
The <span id="Convergence Theorem-Policy Gradient Theorem"></span><strong id="Policy Gradient Theorem">Policy Gradient Theorem</strong> states that the above update rule will converge to a local maxima of the \(v_\pi(s_0) = \mathbb{E}[G_0 \mid s_0]\).
In particular, that
</p>
\[
    \nabla v_\pi(s_0) \propto \sum_s \mu(s) \sum_a v_\pi(s, a) \nabla \pi(a \mid s, \theta)
\]
<p>
again, note that the \(\mu(s)\) term is applied implicitely by the gradient sampling algorithm.
</p>

<div id="Remarks"><h2 id="Remarks" class="header"><a href="#Remarks">Remarks</a></h2></div>

<p>
One major advantage of PGMs is that they learn <em>smoothly</em>, where the policy changes are continuous.
This is in contrast to estimating a value function and having an \(\mathrm{argmax}\) policy (or \(\epsilon\)-greedy).
A major consequence is that it may learn stochastic policies.
</p>

<p>
Further, as it (generally) skips the need for estimating the value function, it does not learn as much about the environment.
This may be beneficial, as it can ignore irrelevant details, or harmful where it discards useful information.
</p>

<p>
<a href="Reinforcement Learning.html">Home</a> <a href="Chapter One.html">1</a> <a href="Chapter Two.html">2</a> <a href="Chapter Three.html">3</a> <a href="Chapter Four.html">4</a><a href="Eligibility Traces.html">(b)</a> <a href="Chapter Five.html">5</a> <a href="Chapter Six.html">6</a>
</p>

        </div>
    </body>
