<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" href="assets/css/bootstrap.min.css ">
        <link rel="stylesheet" href="assets/css/code.min.css">
        <link rel="stylesheet" href="assets/css/atom-one-dark-reasonable.css">
        <link rel="stylesheet" href="assets/css/style.css" />
        <link rel='stylesheet' href='assets/css/roboto.css'>
        <script type="text/javascript" src="assets/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <title>Chapter Five</title>
    </head>
    <body>
        <div class="container">
            
<div id="Function Approximation"><h2 id="Function Approximation" class="header"><a href="#Function Approximation">Function Approximation</a></h2></div>

<p>
We shall now relax the assumption that we may tabulate each state-action pair in a map.
In particular, we now apply the techniques of stochastic gradient descent (SGD).
</p>

<p>
We first describe our value and error functions.
</p>

<p>
For the sake of generality, we shall parameterize \(\hat{v}(s, w)\) instead of specifying it directly.
For example, we may have a linear function \(\hat{v} = x(s)^Tw\), or even a neural network.
</p>

<p>
Then, we shall try to reach the <em>VE Optimum</em>, or "Value Error Optimum", which is the minimum of
</p>
\[
    \mathrm{VE}(w) = \sum_{s \in \mathcal{S}} \mu(s) [ v_pi(s) - \hat{v}(s, w) ]^2
\]
<p>
the least squares error between our estimate and the real value function.
</p>

<p>
Note that we shall weight it by \(\mu(s)\), or the frequency of visiting a state \(s\).
</p>

<div id="SGD"><h2 id="SGD" class="header"><a href="#SGD">SGD</a></h2></div>

<p>
Now, we apply the SGD algorithm to minimize the squared error.
However, as we cannot directly observe \(v_\pi(s)\), we shall use another <em>estimate</em>, namely \(U_t\).
</p>

<p>
If \(\mathbb{E}[U_t|s] = v_\pi(s)\), or when \(U_t\) is an unbiased estimator, then we will reach the optimum.
However, for the remainder of this article, we shall <em>bootstrap</em>, 
</p>
\[
    U_t = \sum_{k=0}^{n-1} \gamma^k g_{t + k} + \gamma^n \hat{v}(s_{t + n})
\]

<p>
and thus, our update rule becomes
</p>
\begin{align}
    w   &amp;\xleftarrow{} w + \alpha \nabla_w (U_t - \hat{v}(s_t))^2 \\
    w   &amp;\xleftarrow{} w + \alpha' (U_t - \hat{v}(s_t)) \nabla_w \hat{v}(s_t) \\
        &amp;\xleftarrow{} w + \alpha' (\sum_{k=0}^{n-1} \gamma^k g_{t + k} + \gamma^n \hat{v}(s_{t + n}) - \nabla_w \hat{v}(s_t))
\end{align}

<p>
Further, when we use a linear value function \(\hat{v}\) and setting \(n=1\), then
</p>
\begin{align}
    w   &amp;\xleftarrow{} w + \alpha (U_t - \hat{v}(s_t)) \nabla_w \hat{v}(s_t) \\
        &amp;\xleftarrow{} w + \alpha (x(s)^Tw - \hat{v}(s_t)) x(s)
\end{align}

<p>
Also, recall that because we are sampling based on trajectories, the \(\mu(s)\) term in error function is implicit.
</p>

<div id="Remarks"><h2 id="Remarks" class="header"><a href="#Remarks">Remarks</a></h2></div>

<p>
The simple SGD algorithm applied to TD learning aas shown above assumes that \(\mathcal{A}(s)\) isn't large.
If it is large, or even continuous, then we need to apply <em>Policy Gradient Methods</em>, see <a href="Chapter Six.html">Chapter Six</a>.
</p>

<p>
Finally, we describe certain ways to create the feature vector \(x(S)\)
</p>
<ul>
<li>
We may bucket the state space into a finite amount of bins, bitvector

<li>
We may sample \(m\) points in our state space, and then apply a normalized distanced based weighting metric (closer is higher weight, softmax?)

</ul>
<p>
<a href="Reinforcement Learning.html">Home</a> <a href="Chapter One.html">1</a> <a href="Chapter Two.html">2</a> <a href="Chapter Three.html">3</a> <a href="Chapter Four.html">4</a><a href="Eligibility Traces.html">(b)</a> <a href="Chapter Five.html">5</a> <a href="Chapter Six.html">6</a> <a href="Chapter Six.html">Next</a>
</p>

        </div>
    </body>
