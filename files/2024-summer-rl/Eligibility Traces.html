<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" href="assets/css/bootstrap.min.css ">
        <link rel="stylesheet" href="assets/css/code.min.css">
        <link rel="stylesheet" href="assets/css/atom-one-dark-reasonable.css">
        <link rel="stylesheet" href="assets/css/style.css" />
        <link rel='stylesheet' href='assets/css/roboto.css'>
        <script type="text/javascript" src="assets/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <title>Eligibility Traces</title>
    </head>
    <body>
        <div class="container">
            
<p>
Note that this is not part of the <a href="https://www.youtube.com/playlist?list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr">Playlist</a>.
See the <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">textbook</a>, chapter 7 instead.
</p>

<div id="Eligibility Traces"><h2 id="Eligibility Traces" class="header"><a href="#Eligibility Traces">Eligibility Traces</a></h2></div>

<p>
In <a href="Chapter Four.html">Chapter Four</a> we discussed \(n\)-step TD algorithms, where we take the <em>trace</em> of the next \(n\) rewards for a state and accumulating into a single update.
</p>

<p>
However, we may combine multiple traces, for each \(n\).
In particular, we shall combine <em>all</em> possible traces, weighting geometrically by a \(\lambda\) parameter.
</p>

<p>
For example, for a reward \(r_t\), we shall acredit it to the states preceding it.
</p>
<ul>
<li>
\(a_t\) gets \(\lambda\) of the credit

<li>
\(a_{t-1}\) gets \(\lambda^2\) of the credit

<li>
\(\ldots\)

</ul>
<p>
This allows for a smoother convergence (from being similar to MC) while also being quick to train (being similar to TD).
Note that choosing lambda, it may approach MC (\(\lambda = 1\)), or it may approach \(1\)-step TD (\(\lambda = 0\)).
</p>

<div id="Tabular and Continuous Case"><h2 id="Tabular and Continuous Case" class="header"><a href="#Tabular and Continuous Case">Tabular and Continuous Case</a></h2></div>

<p>
A state \(s\) is <em>eligibile</em> to receive accreditation for a reward when it was visited.
In particular (for accumulating traces), we define an \(E\) vector, s.t.
</p>
\begin{align}
    E(s_t) &amp;\xleftarrow{} \gamma \lambda (E(s_t) + 1)  \\
    E(s \ne s_t) &amp;\xleftarrow{} \gamma \lambda E(s)  \\
\end{align}

<p>
Thus, we have the following algorithm:
</p>
<ul>
<li>
Initialize \(E(s) \xleftarrow{} 0, \forall s\)

<li>
Repeat for each step of each episode

<ul>
<li>
Update \(E(S)\)

<li>
\(\delta \xleftarrow{} g^{(m)}_t - v(s^{(m)}_t)\)

<li>
\(v(s^{(m)}_t) \gets v(s^{(m)}_t) + \alpha \delta E(s^{(m)}_t)\)

</ul>
</ul>
<p>
In the continuous case, we do not accumulate eligiblity for states, as each individual state has no impact.
Thus, we only decay traces.
</p>

<ul>
<li>
\(E \xleftarrow{} 0\)

<li>
Repeat for each step of each episode

<ul>
<li>
\(E \gets \gamma \lambda E + \nabla v(s^{(m)}_t, w)\)

<li>
\(\delta \xleftarrow{} g^{(m)}_t - v(s^{(m)}_t, w)\)

<li>
\(w \gets w + \alpha \delta E\)

</ul>
</ul>
<p>
<a href="Reinforcement Learning.html">Home</a> <a href="Chapter One.html">1</a> <a href="Chapter Two.html">2</a> <a href="Chapter Three.html">3</a> <a href="Chapter Four.html">4</a><a href="Eligibility Traces.html">(b)</a> <a href="Chapter Five.html">5</a> <a href="Chapter Six.html">6</a>
</p>

        </div>
    </body>
