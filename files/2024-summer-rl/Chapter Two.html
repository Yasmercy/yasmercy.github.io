<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <link rel="stylesheet" href="assets/css/bootstrap.min.css ">
        <link rel="stylesheet" href="assets/css/code.min.css">
        <link rel="stylesheet" href="assets/css/atom-one-dark-reasonable.css">
        <link rel="stylesheet" href="assets/css/style.css" />
        <link rel='stylesheet' href='assets/css/roboto.css'>
        <script type="text/javascript" src="assets/mathjax/es5/tex-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <title>Chapter Two</title>
    </head>
    <body>
        <div class="container">
            
<div id="Review"><h2 id="Review" class="header"><a href="#Review">Review</a></h2></div>
<p>
Last time, we defined the following functions
</p>
<ul>
<li>
\(\pi(a \mid s)\), our policy

<li>
\(v_\pi(s)\), the state value function

<li>
\(v_\pi(s, a)\), the state value function

</ul>
<p>
and mentioned that we want to maximize the expectation of the discounted reward,
</p>

\[
    G^{(\pi)}_t = \sum_{k=t+1}^T \gamma^{k - {t + 1}} R_k 
\]

<div id="Bellman Equations"><h2 id="Bellman Equations" class="header"><a href="#Bellman Equations">Bellman Equations</a></h2></div>
<p>
Recall the example from the previous chapter with \(3\) states and \(2\) actions.
</p>

<p>
Now, let our policy \(\pi\) and suppose that we are on \(s_0\).
If we knew \(v_\pi(s)\) for \(s \ne s_0\), then how shall we find \(v_\pi(s_0)\)?
</p>

<p>
This is a simple application of the expectation formula
</p>
\[
    \mathbb{E}[X] = \sum_{x \in S_X} xf(x) 
\]

<p>
In particular, 
</p>
\[
    v_\pi(s_0) = \sum_r r \cdot p(r \mid s_0, \pi(s_0)) + \gamma \sum_{s} v_\pi(s) p(s \mid s_0, \pi(s_0))
\]

<p>
or that we find the expected reward with the discounted expected next reward.
</p>

<p>
Similarly, we find the values for \(v_\pi(s_0, a_0)\).
</p>

\begin{align}
    v_\pi(s_0) &amp;= \sum_a \pi(a \mid s_0) v_\pi(s_0, a) \\
    v_\pi(s_0, a_0) &amp;= \sum_{s, r} p(s, r \mid s_0, a_0)[r + \gamma v_\pi(s)]
\end{align}

<p>
Finally, we achieve the <em>Bellman equations</em> implicitely with a substitution, which describes \(v_\pi(s)\) and \(v_\pi(s, a)\).
</p>

<div id="Bellman Optimality"><h2 id="Bellman Optimality" class="header"><a href="#Bellman Optimality">Bellman Optimality</a></h2></div>
<p>
We define an optimal policy \(\pi_*\) as the policy that maximizes reward.
In particular,
</p>
\begin{align}
    v_*(s) &amp;= \mathrm{max}_\pi v_\pi(s) = \mathrm{max}_a v_*(s, a) \\
    v_*(s, a) &amp;= \mathrm{max}_\pi v_\pi(s, a)
    
\end{align}

<div id="(Generalized) Policy Iteration"><h2 id="(Generalized) Policy Iteration" class="header"><a href="#(Generalized) Policy Iteration">(Generalized) Policy Iteration</a></h2></div>
<p>
With all this setup, we can describe an algorithm (sequential policy iteration) to find the optimal policies \(\pi_*\), consisting of two subroutines.
</p>

<p>
First, we describe <em>policy evaluation</em>, computing all value functions for a given \(\pi\) (by iterating until equilibrium).
Supposing we have an initial distribution of \(v_\pi(s, a)\) and \(v_\pi(s)\), we shall apply the Bellman equation, updating all values.
Next, we describe <em>policy iteration</em>, which updates each policy \(\pi\) to take the <em>argmax</em> of the value functions.
</p>

<p>
The generalized policy iteration (GPI) theorem states that this process converges to an optimal policy.
It further asserts that this algorithm has redundant steps.
For example, <em>value iteration</em> instead loops over all action pairs, rather than that specified by a policy.
</p>

<div id="Gambler's Problem"><h2 id="Gambler's Problem" class="header"><a href="#Gambler's Problem">Gambler's Problem</a></h2></div>
<p>
A python implementation of the <a href="Gambler.html">Gambler's Problem</a>, described in Sutton-Bartol Example 4.3
</p>

<p>
<a href="Reinforcement Learning.html">Home</a> <a href="Chapter One.html">1</a> <a href="Chapter Two.html">2</a> <a href="Chapter Three.html">3</a> <a href="Chapter Four.html">4</a><a href="Eligibility Traces.html">(b)</a> <a href="Chapter Five.html">5</a> <a href="Chapter Six.html">6</a> <a href="Chapter Three.html">Next</a>
</p>

        </div>
    </body>
